# UNESCO AI Scoring API (FastAPI-enabled)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from fastapi.responses import FileResponse
import pandas as pd
import json
import os
from fpdf import FPDF

app = FastAPI(title="UNESCO AI Ethics Scoring API")

RECOMMENDATION_OUTPUT_PATH = "recommendations_output.json"
RECOMMENDATION_OUTPUT_HTML = "recommendations_report.html"
RECOMMENDATION_OUTPUT_XLSX = "recommendations_report.xlsx"
RECOMMENDATION_OUTPUT_PDF = "recommendations_report.pdf"

# CONTROL_SET and RECOMMENDATIONS must be inserted here manually or imported from external module
CONTROL_SET = []  # Placeholder: insert actual dataset here
RECOMMENDATIONS = {}  # Placeholder: insert actual mapping here

class ControlEvidence(BaseModel):
    control_id: str
    score: int
    notes: Optional[str] = None

class AuditSubmission(BaseModel):
    system_name: str
    submitted_by: str
    responses: List[ControlEvidence]

class AuditResult(BaseModel):
    system_name: str
    submitted_by: str
    overall_score: float
    max_score: int
    detailed_breakdown: List[dict]
    recommendations: List[dict]

def generate_pdf_report(data: List[dict], filepath: str, overall_score: float, system_name: str):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)

    pdf.add_page()
    pdf.set_font("Arial", "B", 22)
    pdf.cell(0, 30, "AI Ethics Benchmarking Report", ln=True, align="C")
    pdf.set_font("Arial", size=14)
    pdf.ln(10)
    pdf.cell(0, 10, f"System: {system_name}", ln=True, align="C")
    pdf.cell(0, 10, f"Overall Score: {overall_score:.2f}%", ln=True, align="C")
    pdf.cell(0, 10, "Generated by Kloudstack", ln=True, align="C")
    pdf.cell(0, 10, "www.kloudstack.co.uk", ln=True, align="C")

    pdf.add_page()
    pdf.set_font("Arial", "B", 16)
    pdf.cell(0, 10, "Summary of Findings", ln=True)
    pdf.set_font("Arial", size=12)
    pdf.ln(5)
    pdf.multi_cell(0, 8, f"The system '{system_name}' scored {overall_score:.2f}% based on ethical AI principles defined by UNESCO. Below are domain-specific recommendations for areas scoring below compliance thresholds.")

    pdf.set_font("Arial", "B", 14)
    pdf.ln(10)
    pdf.cell(0, 10, "Recommendations", ln=True)
    pdf.set_font("Arial", size=10)
    for item in data:
        pdf.set_font("Arial", "B", 11)
        pdf.cell(0, 10, f"Domain: {item['domain']}", ln=True)
        pdf.set_font("Arial", size=10)
        pdf.multi_cell(0, 8, f"Severity: {item['severity']}")
        pdf.multi_cell(0, 8, f"EN: {item['recommendation_en']}")
        pdf.multi_cell(0, 8, f"AR: {item['recommendation_ar']}")
        pdf.ln(4)

    pdf.output(filepath)

def score_system(submission: AuditSubmission):
    control_map = {c["control_id"]: c for c in CONTROL_SET}
    total_score = 0
    max_score = 0
    breakdown = []
    submitted_controls = {item.control_id for item in submission.responses}

    missing_controls = [c for c in CONTROL_SET if c["control_id"] not in submitted_controls]
    if missing_controls:
        missing_ids = [c["control_id"] for c in missing_controls]
        raise HTTPException(status_code=400, detail=f"Missing scores for control IDs: {missing_ids}")

    low_scoring_domains = set()
    for item in submission.responses:
        control = control_map.get(item.control_id)
        if not control:
            raise HTTPException(status_code=400, detail=f"Invalid control ID: {item.control_id}")

        weight = control.get("impact_weight", 1)
        weighted_score = item.score * weight
        total_score += weighted_score
        max_score += 5 * weight

        if item.score < 3:
            low_scoring_domains.add(control["domain"])

        breakdown.append({
            "control_id": item.control_id,
            "principle": control["principle"],
            "domain": control["domain"],
            "score": item.score,
            "weight": weight,
            "weighted_score": weighted_score,
            "notes": item.notes
        })

    overall_score = round((total_score / max_score) * 100, 2) if max_score else 0.0
    recommendation_list = []
    for d in sorted(low_scoring_domains):
        if d in RECOMMENDATIONS:
            r = RECOMMENDATIONS[d]
            recommendation_list.append({
                "domain": d,
                "severity": r["severity"],
                "recommendation_en": r["en"],
                "recommendation_ar": r["ar"]
            })

    pd.DataFrame(recommendation_list).to_excel(RECOMMENDATION_OUTPUT_XLSX, index=False)
    pd.DataFrame(recommendation_list).to_html(RECOMMENDATION_OUTPUT_HTML)
    with open(RECOMMENDATION_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(recommendation_list, f, indent=2, ensure_ascii=False)
    generate_pdf_report(recommendation_list, RECOMMENDATION_OUTPUT_PDF, overall_score, submission.system_name)

    return AuditResult(
        system_name=submission.system_name,
        submitted_by=submission.submitted_by,
        overall_score=overall_score,
        max_score=max_score,
        detailed_breakdown=breakdown,
        recommendations=recommendation_list
    )

@app.post("/score", response_model=AuditResult)
def submit_audit(submission: AuditSubmission):
    return score_system(submission)

@app.get("/download/pdf")
def get_pdf():
    if os.path.exists(RECOMMENDATION_OUTPUT_PDF):
        return FileResponse(RECOMMENDATION_OUTPUT_PDF, media_type="application/pdf")
    raise HTTPException(status_code=404, detail="PDF not found")
