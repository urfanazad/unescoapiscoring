# UNESCO AI Scoring Script (CLI-compatible, no FastAPI)

import json
import os
import pandas as pd
from typing import List, Optional
from dataclasses import dataclass
from io import BytesIO
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer

# Output paths
RECOMMENDATION_OUTPUT_PATH = "recommendations_output.json"
RECOMMENDATION_OUTPUT_HTML = "recommendations_report.html"
RECOMMENDATION_OUTPUT_CSV = "recommendations_report.csv"
RECOMMENDATION_OUTPUT_PDF = "recommendations_report.pdf"

# Sample control and recommendation data (should be replaced with full sets)
CONTROL_SET = [
    {"control_id": "UNESCO-P01-C01", "principle": "Human Rights", "domain": "Fairness", "impact_weight": 3},
    {"control_id": "UNESCO-P01-C02", "principle": "Privacy", "domain": "Transparency", "impact_weight": 2}
]

RECOMMENDATIONS = {
    "Fairness": {"severity": "High", "en": "Improve fairness metrics.", "ar": "تحسين مقاييس العدالة."},
    "Transparency": {"severity": "Medium", "en": "Enhance transparency protocols.", "ar": "تعزيز بروتوكولات الشفافية."}
}

@dataclass
class ControlEvidence:
    control_id: str
    score: int
    notes: Optional[str] = None

@dataclass
class AuditSubmission:
    system_name: str
    submitted_by: str
    responses: List[ControlEvidence]

def generate_pdf_report(data: List[dict], filepath: str, overall_score: float, system_name: str):
    styles = getSampleStyleSheet()
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    story = []

    story.append(Paragraph("AI Ethics Benchmarking Report", styles['Title']))
    story.append(Spacer(1, 12))
    story.append(Paragraph(f"System: {system_name}", styles['Heading2']))
    story.append(Paragraph(f"Overall Score: {overall_score:.2f}%", styles['Heading2']))
    story.append(Paragraph("Generated by Kloudstack", styles['Normal']))
    story.append(Paragraph("www.kloudstack.co.uk", styles['Normal']))
    story.append(Spacer(1, 24))

    story.append(Paragraph("Summary of Findings", styles['Heading2']))
    story.append(Spacer(1, 6))
    story.append(Paragraph(
        f"The system '{system_name}' scored {overall_score:.2f}% based on ethical AI principles defined by UNESCO. ",
        styles['BodyText']
    ))

    story.append(Spacer(1, 12))
    story.append(Paragraph("Recommendations", styles['Heading2']))

    for item in data:
        story.append(Spacer(1, 10))
        story.append(Paragraph(f"<b>Domain:</b> {item['domain']}", styles['BodyText']))
        story.append(Paragraph(f"<b>Severity:</b> {item['severity']}", styles['BodyText']))
        story.append(Paragraph(f"<b>EN:</b> {item['recommendation_en']}", styles['BodyText']))
        story.append(Paragraph(f"<b>AR:</b> {item['recommendation_ar']}", styles['BodyText']))

    doc.build(story)
    with open(filepath, 'wb') as f:
        f.write(buffer.getvalue())

def score_system(submission: AuditSubmission):
    control_map = {c["control_id"]: c for c in CONTROL_SET}
    total_score = 0
    max_score = 0
    breakdown = []
    submitted_controls = {item.control_id for item in submission.responses}

    missing_controls = [c for c in CONTROL_SET if c["control_id"] not in submitted_controls]
    if missing_controls:
        raise ValueError(f"Missing scores for control IDs: {[c['control_id'] for c in missing_controls]}")

    low_scoring_domains = set()
    for item in submission.responses:
        control = control_map.get(item.control_id)
        if not control:
            raise ValueError(f"Invalid control ID: {item.control_id}. Available IDs: {list(control_map.keys())}")

        weight = control.get("impact_weight", 1)
        weighted_score = item.score * weight
        total_score += weighted_score
        max_score += 5 * weight

        if item.score < 3:
            low_scoring_domains.add(control["domain"])

        breakdown.append({
            "control_id": item.control_id,
            "principle": control["principle"],
            "domain": control["domain"],
            "score": item.score,
            "weight": weight,
            "weighted_score": weighted_score,
            "notes": item.notes
        })

    overall_score = round((total_score / max_score) * 100, 2) if max_score else 0.0

    recommendation_list = []
    for d in sorted(low_scoring_domains):
        if d in RECOMMENDATIONS:
            r = RECOMMENDATIONS[d]
            recommendation_list.append({
                "domain": d,
                "severity": r["severity"],
                "recommendation_en": r["en"],
                "recommendation_ar": r["ar"]
            })

    # Export outputs (no Excel due to missing openpyxl)
    pd.DataFrame(recommendation_list).to_csv(RECOMMENDATION_OUTPUT_CSV, index=False)
    pd.DataFrame(recommendation_list).to_html(RECOMMENDATION_OUTPUT_HTML)
    with open(RECOMMENDATION_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(recommendation_list, f, indent=2, ensure_ascii=False)

    generate_pdf_report(recommendation_list, RECOMMENDATION_OUTPUT_PDF, overall_score, submission.system_name)

    return {
        "system_name": submission.system_name,
        "submitted_by": submission.submitted_by,
        "overall_score": overall_score,
        "max_score": max_score,
        "detailed_breakdown": breakdown,
        "recommendations": recommendation_list
    }

# Example usage:
if __name__ == "__main__":
    example = AuditSubmission(
        system_name="Demo System",
        submitted_by="Admin",
        responses=[
            ControlEvidence(control_id="UNESCO-P01-C01", score=4),
            ControlEvidence(control_id="UNESCO-P01-C02", score=2),
        ]
    )
    result = score_system(example)
    print(json.dumps(result, indent=2, ensure_ascii=False))
